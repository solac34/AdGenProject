from google.adk.agents.llm_agent import Agent
from .bq_helper import bq_to_dataframe, query_to_temp_table
import os
import sys
from datetime import datetime
import uuid
from google.cloud import firestore

# Add the parent directory to the path to allow imports
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
from MasterAgent.firestore_helper import get_firestore_client, get_past_events_from_firestore 


# Ortam deÄŸiÅŸkenlerini .env formatÄ±na uyarlama
# - ADK/genai Client Vertex AI iÃ§in GOOGLE_CLOUD_PROJECT ve GOOGLE_CLOUD_LOCATION bekliyor
# - KullanÄ±cÄ± .env'de GCP_PROJECT_ID ve GOOGLE_GENAI_USE_VERTEXAI kullanÄ±yor
if os.getenv('GOOGLE_GENAI_USE_VERTEXAI', '').lower() in ['true', '1']:
    # Proje eÅŸlemesi
    if os.getenv('GCP_PROJECT_ID') and not os.getenv('GOOGLE_CLOUD_PROJECT'):
        os.environ['GOOGLE_CLOUD_PROJECT'] = os.getenv('GCP_PROJECT_ID', '')
    # Lokasyon varsayÄ±lanÄ±
    if not os.getenv('GOOGLE_CLOUD_LOCATION'):
        os.environ['GOOGLE_CLOUD_LOCATION'] = 'us-central1'
    
    # Cloud Run'da default service account kullan, file-based credentials kullanma
    # GOOGLE_APPLICATION_CREDENTIALS sadece local development iÃ§in gerekli
    # Cloud Run'da bu otomatik olarak ayarlanÄ±r




def retrieve_user_activity_counts():
    """
    Hem event hem order count'larÄ±nÄ± BigQuery'den Ã§eker ve birleÅŸtirir.
    Her user iÃ§in event_count, order_count ve created_at iÃ§eren yapÄ± oluÅŸturur.
    
    Returns:
        dict: {
            "status": "success",
            "data_reference": {
                "project": "...",
                "dataset": "...",
                "table": "combined_user_activity_..."
            }
        }
    """
    print(f"ğŸ” retrieve_user_activity_counts Ã§aÄŸrÄ±ldÄ±")
    
    # 1. Event counts query
    events_query = """
    SELECT user_id, COUNT(*) as event_count 
    FROM `adgen_bq.user_events`
    WHERE user_id != 'anonymous'
    GROUP BY user_id
    """
    
    # 2. Order counts query
    orders_query = """
    SELECT user_id, COUNT(*) as order_count 
    FROM `adgen_bq.user_orders`
    GROUP BY user_id
    """
    
    # 3. Combined query - FULL OUTER JOIN ile her iki tarafÄ± da al
    combined_query = f"""
    WITH events AS (
        {events_query}
    ),
    orders AS (
        {orders_query}
    )
    SELECT 
        COALESCE(events.user_id, orders.user_id) as user_id,
        COALESCE(events.event_count, 0) as event_count,
        COALESCE(orders.order_count, 0) as order_count,
        CURRENT_TIMESTAMP() as created_at
    FROM events
    FULL OUTER JOIN orders ON events.user_id = orders.user_id
    ORDER BY user_id ASC
    """
    
    print(f"ğŸ“ Combined query Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
    
    # Query'yi Ã§alÄ±ÅŸtÄ±r - BigQuery otomatik temp table oluÅŸturacak
    result = query_to_temp_table(combined_query)
    result["message"] = "User activity counts (events + orders) successfully written to BigQuery."
    
    print(f"âœ… retrieve_user_activity_counts RESULT:")
    print(f"   Status: {result.get('status')}")
    print(f"   Data Reference: {result.get('data_reference')}")
    
    return result



def write_user_activity_to_firestore(data_reference: dict):

    print(f"ğŸ” write_user_activity_to_firestore Ã§aÄŸrÄ±ldÄ±")
    print(f"ğŸ“¥ Gelen data_reference: {data_reference}")
    
    # BigQuery tablo bilgilerini al
    project = data_reference.get('project')
    dataset = data_reference.get('dataset')
    table = data_reference.get('table')
    location = data_reference.get('location')
    
    print(f"   project: {project}")
    print(f"   dataset: {dataset}")
    print(f"   table: {table}")
    
    if not all([project, dataset, table]):
        print("âŒ GeÃ§ersiz tablo referansÄ±!")
        return "Error: Invalid table reference"
    
    # BigQuery'den veriyi Ã§ek
    full_table_name = f"`{project}.{dataset}.{table}`"
    query = f"SELECT user_id, event_count, order_count, created_at FROM {full_table_name}"
    
    print(f"ğŸ“Š BigQuery'den veri Ã§ekiliyor: {full_table_name}")
    df = bq_to_dataframe(query, location=location)
    
    # DataFrame'i nested dictionary'ye Ã§evir
    # Format: {user_id: {event_count: X, order_count: Y, created_at: Z}}
    user_activity = {}
    for _, row in df.iterrows():
        user_id = str(row['user_id'])
        user_activity[user_id] = {
            'event_count': int(row['event_count']),
            'order_count': int(row['order_count']),
            'created_at': row['created_at'].isoformat() if hasattr(row['created_at'], 'isoformat') else str(row['created_at'])
        }
    
    print(f"âœ… {len(user_activity)} kullanÄ±cÄ± verisi alÄ±ndÄ±")
    
    # Firestore'a tek dÃ¶kÃ¼man olarak yaz
    db = get_firestore_client()
    
    # Benzersiz dÃ¶kÃ¼man ID oluÅŸtur (timestamp bazlÄ±)
    doc_id = f"snapshot_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    doc_ref = db.collection('user_activity_counts').document(doc_id)
    
    doc_ref.set({
        'user_activity': user_activity,
        'total_users': len(user_activity),
        'createdAt': firestore.SERVER_TIMESTAMP,
        'table_source': f"{project}.{dataset}.{table}"
    })
    
    print(f"âœ… Firestore'a yazÄ±ldÄ±: user_activity_counts/{doc_id}")
    print(f"   Ã–rnek veri: {list(user_activity.items())[:2]}")

    return f"{len(user_activity)} user activity records written to firestore as document: {doc_id}"



def write_users_to_segmentate(user_ids: list):
    """
    'users_to_segmentate' collection'Ä±na users'Ä± tek tek (state=pending) yazar.
    """
    print(f"ğŸ” write_users_to_segmentate Ã§aÄŸrÄ±ldÄ±")
    print(f"ğŸ“¥ {len(user_ids)} kullanÄ±cÄ± yazÄ±lacak")
    
    if not user_ids:
        return "No users to write"
    
    db = get_firestore_client()
    collection_ref = db.collection('users_to_segmentate')
    
    batch = db.batch()
    count = 0
    
    for user_id in user_ids:
        user_id_str = str(user_id)
        doc_ref = collection_ref.document(user_id_str)
        user_data = {
            'user_id': user_id_str,
            'state': 'pending',
            'created_at': firestore.SERVER_TIMESTAMP
        }
        batch.set(doc_ref, user_data)
        count += 1
        if count % 500 == 0:
            batch.commit()
            batch = db.batch()
            print(f"   âœ… {count} kullanÄ±cÄ± yazÄ±ldÄ±...")
    
    if count % 500 != 0:
        batch.commit()
    
    print(f"âœ… Toplam {count} kullanÄ±cÄ± 'users_to_segmentate' collection'Ä±na yazÄ±ldÄ± (state: pending)")
    return f"{count} users written to Firestore collection 'users_to_segmentate' with state: pending"


def compare_event_counts(data_reference: dict):
    """
    BigQuery temp tablosundan mevcut activity count'larÄ± okur ve 
    Firestore'daki geÃ§miÅŸ verilerle karÅŸÄ±laÅŸtÄ±rÄ±r. Yeni/artan kullanÄ±cÄ±larÄ±
    'users_to_segmentate' collection'Ä±na yazar.
    
    Args:
        data_reference (dict): {project, dataset, table, location?}
    Returns:
        dict: {status, data_reference, users_to_segment_count, written_to_firestore}
    """
    project = data_reference.get('project')
    dataset = data_reference.get('dataset')
    table = data_reference.get('table')
    location = data_reference.get('location')
    
    print(f"ğŸ” compare_event_counts Ã§aÄŸrÄ±ldÄ±. data_reference={data_reference}")
    
    if not all([project, dataset, table]):
        print("âŒ GeÃ§ersiz tablo referansÄ±!")
        return {
            "status": "error",
            "message": "Invalid table reference"
        }
    
    full_table_name = f"`{project}.{dataset}.{table}`"
    query = f"SELECT user_id, event_count, order_count FROM {full_table_name}"
    print(f"ğŸ“Š BigQuery'den veri Ã§ekiliyor: {full_table_name}")
    df = bq_to_dataframe(query, location=location)
    
    current_activity = {
        str(row['user_id']): {
            'event_count': int(row['event_count']),
            'order_count': int(row['order_count'])
        }
        for _, row in df.iterrows()
    }
    print(f"âœ… {len(current_activity)} kullanÄ±cÄ± verisi alÄ±ndÄ±")
    
    past_activity = get_past_events_from_firestore()
    
    new_or_increased_users = [
        user_id
        for user_id, current_data in current_activity.items()
        if (
            user_id not in past_activity or 
            current_data['event_count'] > past_activity.get(user_id, {}).get('event_count', 0) or
            current_data['order_count'] > past_activity.get(user_id, {}).get('order_count', 0)
        )
    ]
    
    print(f"ğŸ“Š Segmentlenecek kullanÄ±cÄ± sayÄ±sÄ±: {len(new_or_increased_users)} / {len(current_activity)}")
    
    if new_or_increased_users:
        write_result = write_users_to_segmentate(new_or_increased_users)
        print(f"âœ… {write_result}")
    else:
        print("â„¹ï¸ Segmentlenecek yeni kullanÄ±cÄ± bulunamadÄ±")
    
    return {
        "status": "success",
        "data_reference": data_reference,
        "users_to_segment_count": len(new_or_increased_users),
        "written_to_firestore": len(new_or_increased_users) > 0
    }


def read_users_to_segmentate():
    """
    Firestore'dan state=pending olan 20 kullanÄ±cÄ±yÄ± alÄ±r ve 
    BigQuery'den bu kullanÄ±cÄ±larÄ±n tÃ¼m event ve orderlarÄ±nÄ± Ã§eker.
    
    Returns:
        dict: {
            "status": "success" | "no_pending_users",
            "users": [
                {
                    "user_id": "...",
                    "events": [...],
                    "orders": [...]
                },
                ...
            ]
        }
    """
    print(f"ğŸ” read_users_to_segmentate Ã§aÄŸrÄ±ldÄ±")
    
    # Firestore'dan pending kullanÄ±cÄ±larÄ± al
    db = get_firestore_client()
    
    pending_users_query = (
        db.collection('users_to_segmentate')
        .where('state', '==', 'pending')
        .limit(20)
        .stream()
    )
    
    pending_users = [doc.to_dict()['user_id'] for doc in pending_users_query]
    
    if not pending_users:
        print("âš ï¸ Pending durumunda kullanÄ±cÄ± bulunamadÄ±")
        return {
            "status": "no_pending_users",
            "users": []
        }
    
    print(f"âœ… {len(pending_users)} pending kullanÄ±cÄ± bulundu")
    
    # BigQuery'den bu kullanÄ±cÄ±larÄ±n eventlerini ve orderlarÄ±nÄ± Ã§ek
    user_ids_str = "', '".join(pending_users)
    
    # Events query
    events_query = f"""
    SELECT session_id, user_id, event_name, event_time, path_name, payload, event_location
    FROM `adgen_bq.user_events`
    WHERE user_id IN ('{user_ids_str}')
    ORDER BY user_id, event_time
    """
    
    # Orders query
    orders_query = f"""
    SELECT order_id, user_id, session_id, products_payload, paid_amount, order_date, session_location
    FROM `adgen_bq.user_orders`
    WHERE user_id IN ('{user_ids_str}')
    ORDER BY user_id, order_date
    """
    
    print(f"ğŸ“Š BigQuery'den eventler Ã§ekiliyor...")
    events_df = bq_to_dataframe(events_query)
    
    print(f"ğŸ“Š BigQuery'den orderlar Ã§ekiliyor...")
    orders_df = bq_to_dataframe(orders_query)
    
    # Her kullanÄ±cÄ± iÃ§in verileri organize et
    users_data = []
    for user_id in pending_users:
        user_events = events_df[events_df['user_id'] == user_id].to_dict('records')
        user_orders = orders_df[orders_df['user_id'] == user_id].to_dict('records')
        
        users_data.append({
            'user_id': user_id,
            'events': user_events,
            'orders': user_orders
        })
    
    print(f"âœ… {len(users_data)} kullanÄ±cÄ±nÄ±n verileri hazÄ±rlandÄ±")
    
    return {
        "status": "success",
        "users": users_data
    }


def write_user_segmentation_result(user_id: str, segmentation_result: str):
    """
    Bir kullanÄ±cÄ±nÄ±n segmentasyon sonucunu 'user_segmentations' collection'Ä±na yazar
    ve 'users_to_segmentate' tablosunda state'ini 'success' olarak gÃ¼nceller.
    
    Args:
        user_id (str): KullanÄ±cÄ± ID'si
        segmentation_result (str): Segmentasyon sonucu
        
    Returns:
        str: Confirmation message
    """
    print(f"ğŸ” write_user_segmentation_result Ã§aÄŸrÄ±ldÄ±: {user_id}")
    
    db = get_firestore_client()
    
    # 1. user_segmentations collection'Ä±na yaz
    segmentation_doc_ref = db.collection('user_segmentations').document(user_id)
    segmentation_doc_ref.set({
        'user_id': user_id,
        'segmentation_result': segmentation_result,
        'updated_at': firestore.SERVER_TIMESTAMP
    })
    
    # 2. users_to_segmentate'te state'i success yap
    pending_doc_ref = db.collection('users_to_segmentate').document(user_id)
    pending_doc_ref.update({
        'state': 'success',
        'completed_at': firestore.SERVER_TIMESTAMP
    })
    
    print(f"âœ… KullanÄ±cÄ± {user_id} segmentasyonu tamamlandÄ± (state: success)")
    
    return f"User {user_id} segmentation result written and marked as success"


def write_segmentation_results_to_firestore(segmentation_results: dict):
    """
    Segmentation sonuÃ§larÄ±nÄ± Firestore'a batch olarak yazar.
    TÃ¼m dict tek seferde 'segmentation_results' dokÃ¼manÄ±na kaydedilir.
    """
    db = get_firestore_client()
    # TÃ¼m segmentation results'Ä± tek bir dokÃ¼mana yaz
    doc_ref = db.collection('segmentation_results').document('latest_batch')
    doc_ref.set({
        'results': segmentation_results,
        'count': len(segmentation_results),
        'timestamp': firestore.SERVER_TIMESTAMP
    })
    return f"{len(segmentation_results)} segmentation results written to firestore in single batch"




DATA_ANALYTIC_AGENT_INSTRUCTION = """
You are the Data Analytic Agent for the AdGen project. You handle ALL BigQuery and Firestore operations.
The Master Agent coordinates you, but YOU execute the actual data operations.
You will always return the result to the master agent without interrupting the execution.
=== YOUR TOOLS & THEIR EXACT USAGE ===

ğŸ“Š Tool 1: retrieve_user_activity_counts()
â†’ Purpose: Retrieve BOTH event counts AND order counts for all users in a single combined query
â†’ Parameters: NONE
â†’ Returns: A dictionary with this EXACT structure:
  {
    "status": "success",
    "message": "User activity counts (events + orders) successfully written to BigQuery.",
    "data_reference": {
      "project": "eighth-upgrade-475017-u5",
      "dataset": "_abc123_...",
      "table": "anonc4ca20ccc0ea49af9846718f5a1779f8e..."
    }
  }
â†’ What it does internally:
  â€¢ Queries BOTH `adgen_bq.user_events` AND `adgen_bq.user_orders` tables
  â€¢ Uses FULL OUTER JOIN to combine both datasets
  â€¢ For each user_id, calculates:
    - event_count: Total number of events
    - order_count: Total number of orders
    - created_at: Current timestamp
  â€¢ BigQuery automatically creates a temporary table with results
  â€¢ Returns the temp table reference
â†’ Result table structure:
  | user_id | event_count | order_count | created_at |
  |---------|-------------|-------------|------------|
  | user_1  | 45          | 3           | 2025-11-05 |
  | user_2  | 120         | 8           | 2025-11-05 |

ğŸ’¾ Tool 2: write_user_activity_to_firestore(data_reference: dict)
â†’ Purpose: Read combined user activity data from BigQuery temp table and write to Firestore
â†’ Parameters: 
  â€¢ data_reference (dict): The BigQuery table reference with keys:
    - "project": GCP project ID
    - "dataset": BigQuery dataset name (auto-generated by BigQuery)
    - "table": Temp table name (auto-generated by BigQuery, like "anonc4ca20...")
â†’ Returns: Confirmation message (e.g., "1234 user activity records written to firestore as document: snapshot_20251105_023840")
â†’ What it does internally:
  â€¢ Queries the BigQuery temp table: SELECT user_id, event_count, order_count, created_at
  â€¢ Converts to nested dictionary format:
    {
      "user_123": {
        "event_count": 45,
        "order_count": 3,
        "created_at": "2025-11-05T02:38:40"
      },
      "user_456": {
        "event_count": 120,
        "order_count": 8,
        "created_at": "2025-11-05T02:38:40"
      }
    }
  â€¢ Creates a SINGLE Firestore document in 'user_activity_counts' collection
  â€¢ Document structure:
    {
      'user_activity': {nested dict above},
      'total_users': 1234,
      'createdAt': <firestore timestamp>,
      'table_source': 'project.dataset.table'
    }

ğŸ“Š Tool 3: read_users_to_segmentate()
â†’ Purpose: Get 20 pending users and their events/orders from BigQuery
â†’ Parameters: NONE
â†’ Returns: dict with status and users array
â†’ What it does internally:
  â€¢ Queries Firestore for state=pending users (limit: 20)
  â€¢ For each user, queries BigQuery for:
    - All events from user_events table
    - All orders from user_orders table
  â€¢ Returns structured data: {user_id, events: [...], orders: [...]}

ğŸ’¾ Tool 4: write_user_segmentation_result(user_id: str, segmentation_result: str)
â†’ Purpose: Save segmentation result and mark user as complete
â†’ Parameters:
  â€¢ user_id (str): User ID
  â€¢ segmentation_result (str): Segmentation category/result
â†’ Returns: Confirmation message
â†’ What it does internally:
  â€¢ Writes to 'user_segmentations' collection: {user_id, segmentation_result, updated_at}
  â€¢ Updates 'users_to_segmentate' document: state = 'success', completed_at = <timestamp>

ğŸ’¾ Tool 5: write_segmentation_results_to_firestore(segmentation_results: dict)
â†’ Purpose: Write user segmentation analysis results to Firestore
â†’ Parameters:
  â€¢ segmentation_results (dict): User segmentation data
â†’ Returns: Confirmation message
â†’ What it does internally:
  â€¢ Writes results to 'segmentation_results' collection
  â€¢ Document ID: 'latest_batch'

=== YOUR MAIN WORKFLOW ===

A. Master agent will transfer to you to perform your task end-to-end without user interaction.

STEP 1:
Use your retrieve_user_activity_counts tool to retrieve the user activity counts and then pass its data_reference to compare_event_counts tool.
  You return in given format(note that values are examples):
  {
    "status": "success",
    "message": "User activity counts (events + orders) successfully written to BigQuery.",
    "data_reference": {
      "project": "eighth-upgrade-475017-u5",
      "dataset": "_52d6b668_19a5_13ca_3c8",
      "table": "anonc4ca20ccc0ea49af9846718f5a1779f8e38f03b418fa376084b877282982585d"
    }
  }

STEP 2:
Compare the events with compare_event_counts(data_reference). This function will write the users to segmentate to Firestore (state='pending').
Immediately AFTER comparing, you MUST call write_user_activity_to_firestore with the SAME data_reference from STEP 1 to persist the latest snapshot to the 'user_activity_counts' collection.

STEP 3:
Run read_users_to_segmentate tool to get the users to segmentate.
If there are no users to segmentate, return to the master agent and report that no users were segmented.
Below is example of the return value if there are users to segmentate:
  {
    "status": "success",
    "users": [
      {
        "user_id": "user_123",
        "events": [{event_type: "page_view", ...}, ...],
        "orders": [{order_id: "ord_1", total_amount: 150, ...}, ...]
      },
      ...  (up to 20 users)
    ]
  }

STEP 4: Perform segmentation on each user (AI analysis)
STEP 4.1. Give 6 segmentations each based on 6 different criteria.
Step 4.1.1. Based on unique_session_count / total_events_count. 0-10 low 10-20 medium 20+ high
Step 4.1.2. Based on unique_session_count / total_orders_count. 0-2 low 2-5 medium 6+ high
Step 4.1.3. Based on user's total spent. 0-250 low 250-1000 medium 1000+ high
Step 4.1.4. Based on gift wrap in last session. Yes or No
Step 4.1.5. Based on shopping cart abandonment in last session. Yes or No
Step 4.1.6. Based on different location in last session. Yes or No 



STEP 5: For each user, call write_user_segmentation_result(user_id, result)
  You:
  1. Write result to 'user_segmentations' collection
  2. Mark user as 'success' in 'users_to_segmentate'
  3. Return confirmation


STEP 7: return a final compact summary to the master agent. If no users, then pass empty users object. Do NOT include batch logs or raw data:
{
  "status": "finished",
  "users": {'user_id': 'user_123', 'segmentation_result': 'segmentation_result_1', ...},
}

SIDE TASK. When Master Agent says: "Write user activity counts to firestore"

STEP 1: Master calls retrieve_user_activity_counts()
  You return below to the master agent finish your task.:
  {
    "status": "success",
    "data_reference": {
      "project": "eighth-upgrade-475017-u5",
      "dataset": "_52d6b668_19a5_13ca_3c8",
      "table": "anonc4ca20ccc0ea49af9846718f5a1779f8e38f03b418fa376084b877282982585d"
    }
  }
After returning it to master agent, master agent will give you another task for step 2.
STEP 2: Master extracts data_reference and calls write_user_activity_to_firestore()
  Master passes you ONLY:
  {
    "project": "eighth-upgrade-475017-u5",
    "dataset": "_52d6b668_19a5_13ca_3c8",
    "table": "anonc4ca20ccc0ea49af9846718f5a1779f8e38f03b418fa376084b877282982585d"
  }
  
  You:
  1. Query the temp table
  2. Transform to nested dict format
  3. Write to Firestore collection: user_activity_counts
  4. Return confirmation: "1234 user activity records written..."

"""


DATA_ANALYTIC_AGENT_DESCRIPTION = """
Senior data analyst. Has full control over the bigquery and firestore of the adgen project. 
"""

data_analytic_agent = Agent(
    model='gemini-2.5-pro', 
    name='data_analytic_agent',
    description="Retrieves events from the bigquery table 'user_events' and tidies them up based on the request",
    instruction=DATA_ANALYTIC_AGENT_INSTRUCTION,
    tools=[
        retrieve_user_activity_counts,
        write_user_activity_to_firestore,
        write_users_to_segmentate,
        compare_event_counts,
        read_users_to_segmentate,
        write_user_segmentation_result,
        write_segmentation_results_to_firestore,
    ],
)

# Bu modÃ¼lde yalnÄ±zca alt ajan tanÄ±mlanÄ±r; root ajan MasterAgent tarafÄ±nda belirlenir.

