from google.adk.agents.llm_agent import Agent
from .bq_helper import bq_to_dataframe, query_to_temp_table
import os
import sys
from datetime import datetime
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
from MasterAgent.firestore_helper import get_firestore_client, get_past_events_from_firestore
from google.cloud import firestore
import uuid 


# Ortam deÄŸiÅŸkenlerini .env formatÄ±na uyarlama
# - ADK/genai Client Vertex AI iÃ§in GOOGLE_CLOUD_PROJECT ve GOOGLE_CLOUD_LOCATION bekliyor
# - KullanÄ±cÄ± .env'de GCP_PROJECT_ID ve GOOGLE_GENAI_USE_VERTEXAI kullanÄ±yor
if os.getenv('GOOGLE_GENAI_USE_VERTEXAI', '').lower() in ['true', '1']:
    # Proje eÅŸlemesi
    if os.getenv('GCP_PROJECT_ID') and not os.getenv('GOOGLE_CLOUD_PROJECT'):
        os.environ['GOOGLE_CLOUD_PROJECT'] = os.getenv('GCP_PROJECT_ID', '')
    # Lokasyon varsayÄ±lanÄ±
    if not os.getenv('GOOGLE_CLOUD_LOCATION'):
        os.environ['GOOGLE_CLOUD_LOCATION'] = 'us-central1'
    # Vertex AI iÃ§in ayrÄ± credential belirtildiyse onu ADC olarak aktar
    if os.getenv('GOOGLE_APPLICATION_CREDENTIALS_AI'):
        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = os.getenv('GOOGLE_APPLICATION_CREDENTIALS_AI', '')




def retrieve_order_counts(): 

    sql_query = f"""
    SELECT user_id, COUNT(*) as count FROM `adgen_bq.user_orders`
    GROUP BY user_id
    ORDER BY count DESC
    """
    
    print(f"ğŸ” retrieve_order_counts Ã§aÄŸrÄ±ldÄ±")
    
    # Query'yi Ã§alÄ±ÅŸtÄ±r - BigQuery otomatik temp table oluÅŸturacak
    result = query_to_temp_table(sql_query)
    result["message"] = "User order counts successfully written to BigQuery for processing."
    
    print(f"âœ… retrieve_order_counts RESULT:")
    print(f"   Data Reference: {result.get('data_reference')}")
    
    return result


def retrieve_event_counts(): 

    sql_query = f"""
    SELECT user_id, COUNT(*) as count FROM `adgen_bq.user_events`
    WHERE user_id != 'anonymous'
    GROUP BY user_id
    ORDER BY user_id ASC
    """
    
    print(f"ğŸ” retrieve_event_counts Ã§aÄŸrÄ±ldÄ±")
    print(f"ğŸ“ SQL Query: {sql_query[:100]}...")
    
    # Query'yi Ã§alÄ±ÅŸtÄ±r - BigQuery otomatik temp table oluÅŸturacak
    result = query_to_temp_table(sql_query)
    result["message"] = "User event counts successfully written to BigQuery for processing."
    
    print(f"âœ… retrieve_event_counts RESULT:")
    print(f"   Status: {result.get('status')}")
    print(f"   Data Reference: {result.get('data_reference')}")
    
    return result



def write_new_events_to_firestore(data_reference: dict):
    """
    BigQuery temp tablosundan event count'larÄ± okur ve Firestore'a tek bir dÃ¶kÃ¼man olarak yazar.
    
    Args:
        data_reference: BigQuery tablo referansÄ±
            {
                "project": "...",
                "dataset": "...",
                "table": "..."
            }
    
    Returns:
        str: Confirmation message
    """
    print(f"ğŸ” write_new_events_to_firestore Ã§aÄŸrÄ±ldÄ±")
    print(f"ğŸ“¥ Gelen data_reference: {data_reference}")
    print(f"ğŸ“¥ data_reference type: {type(data_reference)}")
    
    # BigQuery tablo bilgilerini al
    project = data_reference.get('project')
    dataset = data_reference.get('dataset')
    table = data_reference.get('table')
    
    print(f"   project: {project}")
    print(f"   dataset: {dataset}")
    print(f"   table: {table}")
    
    if not all([project, dataset, table]):
        print("âŒ GeÃ§ersiz tablo referansÄ±!")
        print(f"   Missing: project={bool(project)}, dataset={bool(dataset)}, table={bool(table)}")
        return "Error: Invalid table reference"
    
    # BigQuery'den veriyi Ã§ek
    full_table_name = f"`{project}.{dataset}.{table}`"
    query = f"SELECT user_id, count FROM {full_table_name}"
    
    print(f"ğŸ“Š BigQuery'den veri Ã§ekiliyor: {full_table_name}")
    df = bq_to_dataframe(query)
    
    # DataFrame'i dictionary'ye Ã§evir
    event_counts = {str(row['user_id']): int(row['count']) for _, row in df.iterrows()}
    
    print(f"âœ… {len(event_counts)} kullanÄ±cÄ± verisi alÄ±ndÄ±")
    
    # Firestore'a tek dÃ¶kÃ¼man olarak yaz
    db = get_firestore_client()
    
    # Benzersiz dÃ¶kÃ¼man ID oluÅŸtur (timestamp bazlÄ±)
    doc_id = f"snapshot_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    doc_ref = db.collection('user_event_counts').document(doc_id)
    
    doc_ref.set({
        'event_counts': event_counts,
        'total_users': len(event_counts),
        'createdAt': firestore.SERVER_TIMESTAMP,
        'table_source': f"{project}.{dataset}.{table}"
    })
    
    print(f"âœ… Firestore'a yazÄ±ldÄ±: user_event_counts/{doc_id}")

    return f"{len(event_counts)} user event counts written to firestore as document: {doc_id}"



def write_segmentation_results_to_firestore(segmentation_results: dict):
    """
    Segmentation sonuÃ§larÄ±nÄ± Firestore'a batch olarak yazar.
    TÃ¼m dict tek seferde 'segmentation_results' dokÃ¼manÄ±na kaydedilir.
    """
    db = get_firestore_client()
    # TÃ¼m segmentation results'Ä± tek bir dokÃ¼mana yaz
    doc_ref = db.collection('segmentation_results').document('latest_batch')
    doc_ref.set({
        'results': segmentation_results,
        'count': len(segmentation_results),
        'timestamp': firestore.SERVER_TIMESTAMP
    })
    return f"{len(segmentation_results)} segmentation results written to firestore in single batch"




DATA_ANALYTIC_AGENT_INSTRUCTION = """
You are the Data Analytic Agent for the AdGen project. You handle ALL BigQuery and Firestore operations.
The Master Agent coordinates you, but YOU execute the actual data operations.

=== YOUR TOOLS & THEIR EXACT USAGE ===

ğŸ“Š Tool 1: retrieve_event_counts()
â†’ Purpose: Query BigQuery for user event counts and create a temporary table
â†’ Parameters: NONE
â†’ Returns: A dictionary with this EXACT structure:
  {
    "status": "success",
    "message": "User event counts successfully written to BigQuery for processing.",
    "data_reference": {
      "project": "eighth-upgrade-475017-u5",
      "dataset": "temp_datasets",
      "table": "user_events_<uuid>"
    }
  }
â†’ What it does internally:
  â€¢ Queries `adgen_bq.user_events` table
  â€¢ Groups by user_id, counts events
  â€¢ Writes results to a temp BigQuery table
  â€¢ Returns the temp table reference

ğŸ“Š Tool 2: retrieve_order_counts()
â†’ Purpose: Query BigQuery for user order counts and create a temporary table
â†’ Parameters: NONE
â†’ Returns: Same structure as retrieve_event_counts() but for orders
â†’ What it does internally:
  â€¢ Queries `adgen_bq.user_orders` table
  â€¢ Groups by user_id, counts orders
  â€¢ Writes results to a temp BigQuery table
  â€¢ Returns the temp table reference

ğŸ’¾ Tool 3: write_new_events_to_firestore(data_reference: dict)
â†’ Purpose: Read from BigQuery temp table and write a snapshot to Firestore
â†’ Parameters: 
  â€¢ data_reference (dict): The BigQuery table reference with keys:
    - "project": GCP project ID
    - "dataset": BigQuery dataset name
    - "table": Temp table name
â†’ Returns: Confirmation message string (e.g., "1234 user event counts written to firestore as document: snapshot_20251105_120000")
â†’ What it does internally:
  â€¢ Queries the BigQuery temp table using the provided reference
  â€¢ Reads all user_id and count pairs
  â€¢ Creates a SINGLE Firestore document in 'user_event_counts' collection
  â€¢ Document structure:
    {
      'event_counts': {user_id: count, ...},
      'total_users': 1234,
      'createdAt': <timestamp>,
      'table_source': 'project.dataset.table'
    }

ğŸ’¾ Tool 4: write_segmentation_results_to_firestore(segmentation_results: dict)
â†’ Purpose: Write user segmentation analysis results to Firestore
â†’ Parameters:
  â€¢ segmentation_results (dict): User segmentation data
â†’ Returns: Confirmation message
â†’ What it does internally:
  â€¢ Writes results to 'segmentation_results' collection
  â€¢ Document ID: 'latest_batch'

=== IMPORTANT WORKFLOW NOTES ===

When Master Agent asks you to:

A. "Run retrieve_event_counts":
   â†’ Execute retrieve_event_counts() with NO parameters
   â†’ Return the ENTIRE result dictionary to Master Agent
   
B. "Run write_new_events_to_firestore with <data>":
   â†’ The Master Agent will pass you ONLY the data_reference object
   â†’ NOT the full result dictionary
   â†’ The data_reference contains: {project, dataset, table}
   â†’ Execute write_new_events_to_firestore(data_reference)
   â†’ Return the confirmation message

=== DATA FLOW EXAMPLE ===

Step 1: Master calls retrieve_event_counts()
  You return:
  {
    "status": "success",
    "data_reference": {
      "project": "eighth-upgrade-475017-u5",
      "dataset": "temp_datasets",
      "table": "user_events_abc123"
    }
  }

Step 2: Master extracts data_reference and calls write_new_events_to_firestore()
  Master passes you ONLY:
  {
    "project": "eighth-upgrade-475017-u5",
    "dataset": "temp_datasets",
    "table": "user_events_abc123"
  }
  
  You query BigQuery, write to Firestore, return confirmation.

=== KEY RULES ===
âœ… Always return complete, well-structured responses
âœ… Include clear confirmation messages with counts and document IDs
âœ… Log progress with print statements (they help debugging)
âœ… Handle errors gracefully and return descriptive error messages
""" 

DATA_ANALYTIC_AGENT_DESCRIPTION = """
Senior data analyst. Has full control over the bigquery and firestore of the adgen project. 
"""

data_analytic_agent = Agent(
    model='gemini-2.5-flash', 
    name='data_analytic_agent',
    description="Retrieves events from the bigquery table 'user_events' and tidies them up based on the request",
    instruction=DATA_ANALYTIC_AGENT_INSTRUCTION,
    tools=[
        retrieve_order_counts, 
        retrieve_event_counts,
        write_new_events_to_firestore,
        write_segmentation_results_to_firestore,
    ],
)

# Bu modÃ¼lde yalnÄ±zca alt ajan tanÄ±mlanÄ±r; root ajan MasterAgent tarafÄ±nda belirlenir.

