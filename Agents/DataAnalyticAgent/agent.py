from google.adk.agents.llm_agent import Agent
from .bq_helper import bq_to_dataframe, query_to_temp_table
import os
import sys
from datetime import datetime
sys.path.insert(0, os.path.dirname(os.path.dirname(__file__)))
from MasterAgent.firestore_helper import get_firestore_client, get_past_events_from_firestore
from google.cloud import firestore
import uuid 


# Ortam deÄŸiÅŸkenlerini .env formatÄ±na uyarlama
# - ADK/genai Client Vertex AI iÃ§in GOOGLE_CLOUD_PROJECT ve GOOGLE_CLOUD_LOCATION bekliyor
# - KullanÄ±cÄ± .env'de GCP_PROJECT_ID ve GOOGLE_GENAI_USE_VERTEXAI kullanÄ±yor
if os.getenv('GOOGLE_GENAI_USE_VERTEXAI', '').lower() in ['true', '1']:
    # Proje eÅŸlemesi
    if os.getenv('GCP_PROJECT_ID') and not os.getenv('GOOGLE_CLOUD_PROJECT'):
        os.environ['GOOGLE_CLOUD_PROJECT'] = os.getenv('GCP_PROJECT_ID', '')
    # Lokasyon varsayÄ±lanÄ±
    if not os.getenv('GOOGLE_CLOUD_LOCATION'):
        os.environ['GOOGLE_CLOUD_LOCATION'] = 'us-central1'
    # Vertex AI iÃ§in ayrÄ± credential belirtildiyse onu ADC olarak aktar
    if os.getenv('GOOGLE_APPLICATION_CREDENTIALS_AI'):
        os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = os.getenv('GOOGLE_APPLICATION_CREDENTIALS_AI', '')




def retrieve_user_activity_counts():
    """
    Hem event hem order count'larÄ±nÄ± BigQuery'den Ã§eker ve birleÅŸtirir.
    Her user iÃ§in event_count, order_count ve created_at iÃ§eren yapÄ± oluÅŸturur.
    
    Returns:
        dict: {
            "status": "success",
            "data_reference": {
                "project": "...",
                "dataset": "...",
                "table": "combined_user_activity_..."
            }
        }
    """
    print(f"ğŸ” retrieve_user_activity_counts Ã§aÄŸrÄ±ldÄ±")
    
    # 1. Event counts query
    events_query = """
    SELECT user_id, COUNT(*) as event_count 
    FROM `adgen_bq.user_events`
    WHERE user_id != 'anonymous'
    GROUP BY user_id
    """
    
    # 2. Order counts query
    orders_query = """
    SELECT user_id, COUNT(*) as order_count 
    FROM `adgen_bq.user_orders`
    GROUP BY user_id
    """
    
    # 3. Combined query - FULL OUTER JOIN ile her iki tarafÄ± da al
    combined_query = f"""
    WITH events AS (
        {events_query}
    ),
    orders AS (
        {orders_query}
    )
    SELECT 
        COALESCE(events.user_id, orders.user_id) as user_id,
        COALESCE(events.event_count, 0) as event_count,
        COALESCE(orders.order_count, 0) as order_count,
        CURRENT_TIMESTAMP() as created_at
    FROM events
    FULL OUTER JOIN orders ON events.user_id = orders.user_id
    ORDER BY user_id ASC
    """
    
    print(f"ğŸ“ Combined query Ã§alÄ±ÅŸtÄ±rÄ±lÄ±yor...")
    
    # Query'yi Ã§alÄ±ÅŸtÄ±r - BigQuery otomatik temp table oluÅŸturacak
    result = query_to_temp_table(combined_query)
    result["message"] = "User activity counts (events + orders) successfully written to BigQuery."
    
    print(f"âœ… retrieve_user_activity_counts RESULT:")
    print(f"   Status: {result.get('status')}")
    print(f"   Data Reference: {result.get('data_reference')}")
    
    return result



def write_user_activity_to_firestore(data_reference: dict):
    """
    BigQuery temp tablosundan user activity verilerini (event_count, order_count, created_at) 
    okur ve Firestore'a tek bir dÃ¶kÃ¼man olarak yazar.
    
    Args:
        data_reference: BigQuery tablo referansÄ±
            {
                "project": "...",
                "dataset": "...",
                "table": "..."
            }
    
    Returns:
        str: Confirmation message
    """
    print(f"ğŸ” write_user_activity_to_firestore Ã§aÄŸrÄ±ldÄ±")
    print(f"ğŸ“¥ Gelen data_reference: {data_reference}")
    
    # BigQuery tablo bilgilerini al
    project = data_reference.get('project')
    dataset = data_reference.get('dataset')
    table = data_reference.get('table')
    
    print(f"   project: {project}")
    print(f"   dataset: {dataset}")
    print(f"   table: {table}")
    
    if not all([project, dataset, table]):
        print("âŒ GeÃ§ersiz tablo referansÄ±!")
        return "Error: Invalid table reference"
    
    # BigQuery'den veriyi Ã§ek
    full_table_name = f"`{project}.{dataset}.{table}`"
    query = f"SELECT user_id, event_count, order_count, created_at FROM {full_table_name}"
    
    print(f"ğŸ“Š BigQuery'den veri Ã§ekiliyor: {full_table_name}")
    df = bq_to_dataframe(query)
    
    # DataFrame'i nested dictionary'ye Ã§evir
    # Format: {user_id: {event_count: X, order_count: Y, created_at: Z}}
    user_activity = {}
    for _, row in df.iterrows():
        user_id = str(row['user_id'])
        user_activity[user_id] = {
            'event_count': int(row['event_count']),
            'order_count': int(row['order_count']),
            'created_at': row['created_at'].isoformat() if hasattr(row['created_at'], 'isoformat') else str(row['created_at'])
        }
    
    print(f"âœ… {len(user_activity)} kullanÄ±cÄ± verisi alÄ±ndÄ±")
    
    # Firestore'a tek dÃ¶kÃ¼man olarak yaz
    db = get_firestore_client()
    
    # Benzersiz dÃ¶kÃ¼man ID oluÅŸtur (timestamp bazlÄ±)
    doc_id = f"snapshot_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    doc_ref = db.collection('user_activity_counts').document(doc_id)
    
    doc_ref.set({
        'user_activity': user_activity,
        'total_users': len(user_activity),
        'createdAt': firestore.SERVER_TIMESTAMP,
        'table_source': f"{project}.{dataset}.{table}"
    })
    
    print(f"âœ… Firestore'a yazÄ±ldÄ±: user_activity_counts/{doc_id}")
    print(f"   Ã–rnek veri: {list(user_activity.items())[:2]}")

    return f"{len(user_activity)} user activity records written to firestore as document: {doc_id}"



def write_segmentation_results_to_firestore(segmentation_results: dict):
    """
    Segmentation sonuÃ§larÄ±nÄ± Firestore'a batch olarak yazar.
    TÃ¼m dict tek seferde 'segmentation_results' dokÃ¼manÄ±na kaydedilir.
    """
    db = get_firestore_client()
    # TÃ¼m segmentation results'Ä± tek bir dokÃ¼mana yaz
    doc_ref = db.collection('segmentation_results').document('latest_batch')
    doc_ref.set({
        'results': segmentation_results,
        'count': len(segmentation_results),
        'timestamp': firestore.SERVER_TIMESTAMP
    })
    return f"{len(segmentation_results)} segmentation results written to firestore in single batch"




DATA_ANALYTIC_AGENT_INSTRUCTION = """
You are the Data Analytic Agent for the AdGen project. You handle ALL BigQuery and Firestore operations.
The Master Agent coordinates you, but YOU execute the actual data operations.

=== YOUR TOOLS & THEIR EXACT USAGE ===

ğŸ“Š Tool 1: retrieve_user_activity_counts()
â†’ Purpose: Retrieve BOTH event counts AND order counts for all users in a single combined query
â†’ Parameters: NONE
â†’ Returns: A dictionary with this EXACT structure:
  {
    "status": "success",
    "message": "User activity counts (events + orders) successfully written to BigQuery.",
    "data_reference": {
      "project": "eighth-upgrade-475017-u5",
      "dataset": "_abc123_...",
      "table": "anonc4ca20ccc0ea49af9846718f5a1779f8e..."
    }
  }
â†’ What it does internally:
  â€¢ Queries BOTH `adgen_bq.user_events` AND `adgen_bq.user_orders` tables
  â€¢ Uses FULL OUTER JOIN to combine both datasets
  â€¢ For each user_id, calculates:
    - event_count: Total number of events
    - order_count: Total number of orders
    - created_at: Current timestamp
  â€¢ BigQuery automatically creates a temporary table with results
  â€¢ Returns the temp table reference
â†’ Result table structure:
  | user_id | event_count | order_count | created_at |
  |---------|-------------|-------------|------------|
  | user_1  | 45          | 3           | 2025-11-05 |
  | user_2  | 120         | 8           | 2025-11-05 |

ğŸ’¾ Tool 2: write_user_activity_to_firestore(data_reference: dict)
â†’ Purpose: Read combined user activity data from BigQuery temp table and write to Firestore
â†’ Parameters: 
  â€¢ data_reference (dict): The BigQuery table reference with keys:
    - "project": GCP project ID
    - "dataset": BigQuery dataset name (auto-generated by BigQuery)
    - "table": Temp table name (auto-generated by BigQuery, like "anonc4ca20...")
â†’ Returns: Confirmation message (e.g., "1234 user activity records written to firestore as document: snapshot_20251105_023840")
â†’ What it does internally:
  â€¢ Queries the BigQuery temp table: SELECT user_id, event_count, order_count, created_at
  â€¢ Converts to nested dictionary format:
    {
      "user_123": {
        "event_count": 45,
        "order_count": 3,
        "created_at": "2025-11-05T02:38:40"
      },
      "user_456": {
        "event_count": 120,
        "order_count": 8,
        "created_at": "2025-11-05T02:38:40"
      }
    }
  â€¢ Creates a SINGLE Firestore document in 'user_activity_counts' collection
  â€¢ Document structure:
    {
      'user_activity': {nested dict above},
      'total_users': 1234,
      'createdAt': <firestore timestamp>,
      'table_source': 'project.dataset.table'
    }

ğŸ’¾ Tool 3: write_segmentation_results_to_firestore(segmentation_results: dict)
â†’ Purpose: Write user segmentation analysis results to Firestore
â†’ Parameters:
  â€¢ segmentation_results (dict): User segmentation data
â†’ Returns: Confirmation message
â†’ What it does internally:
  â€¢ Writes results to 'segmentation_results' collection
  â€¢ Document ID: 'latest_batch'

=== WORKFLOW EXAMPLE ===

When Master Agent says: "Write user activity counts to firestore"

STEP 1: Master calls retrieve_user_activity_counts()
  You return:
  {
    "status": "success",
    "data_reference": {
      "project": "eighth-upgrade-475017-u5",
      "dataset": "_52d6b668_19a5_13ca_3c8",
      "table": "anonc4ca20ccc0ea49af9846718f5a1779f8e38f03b418fa376084b877282982585d"
    }
  }

STEP 2: Master extracts data_reference and calls write_user_activity_to_firestore()
  Master passes you ONLY:
  {
    "project": "eighth-upgrade-475017-u5",
    "dataset": "_52d6b668_19a5_13ca_3c8",
    "table": "anonc4ca20ccc0ea49af9846718f5a1779f8e38f03b418fa376084b877282982585d"
  }
  
  You:
  1. Query the temp table
  2. Transform to nested dict format
  3. Write to Firestore collection: user_activity_counts
  4. Return confirmation: "1234 user activity records written..."

=== KEY ADVANTAGES ===
âœ… Single query retrieves BOTH events and orders (efficient!)
âœ… Combined data structure: {event_count + order_count + created_at}
âœ… Handles users with only events, only orders, or both (FULL OUTER JOIN)
âœ… Timestamp included for tracking when data was retrieved
âœ… Automatic temporary table creation by BigQuery

=== KEY RULES ===
âœ… Always return complete, well-structured responses
âœ… Include clear confirmation messages with counts and document IDs
âœ… Log progress with print statements (they help debugging)
âœ… Handle errors gracefully and return descriptive error messages
âœ… The data_reference you receive will have BigQuery's auto-generated table names
""" 

DATA_ANALYTIC_AGENT_DESCRIPTION = """
Senior data analyst. Has full control over the bigquery and firestore of the adgen project. 
"""

data_analytic_agent = Agent(
    model='gemini-2.5-flash', 
    name='data_analytic_agent',
    description="Retrieves events from the bigquery table 'user_events' and tidies them up based on the request",
    instruction=DATA_ANALYTIC_AGENT_INSTRUCTION,
    tools=[
        retrieve_user_activity_counts,
        write_user_activity_to_firestore,
        write_segmentation_results_to_firestore,
    ],
)

# Bu modÃ¼lde yalnÄ±zca alt ajan tanÄ±mlanÄ±r; root ajan MasterAgent tarafÄ±nda belirlenir.

